Algoritmer og datastrukturer

```
Pensumhefte, 2019
```
```
Magnus Lie Hetland
```

- 2019-10-04 15:41:


# Innledning

```
Dette pensumheftet er en kombinasjon av tre ting:
```
( _i_ ) En forelesningsplan;

( _ii_ ) En pensumliste; og

( _iii_ ) Et mini-kompendium.

```
For hver forelesning er det listet opp hva som er hovedtema, og hvilken del av
pensum forelesningen bygger på. I tillegg har jeg laget en detaljert opplisting av
læringsmålene i faget, for å tydeliggjøre hva det er meningen at dere skal kunne.
Bakerst i heftet er det noen appendiks med ekstra materiale, og det utgjør mini-
kompendie-delen. Sammen med pensumboka, forelesningene og øvingene håper jeg
at dette gir dere det dere trenger for å sette dere inn i stoffet – men det er viktig å
huske på at det er dere som må gjøre jobben med å lære det!
```
```
Her er det ikke bare faktakunnskap man skal tilegne seg; det er ideer som
krever modning over litt tid, og det er derfor viktig at dere begynner å jobbe
tidlig, og at dere jobber jevnt.
```
```
Det er ikke så taktisk å utsette pensumlesningen til eksamensperioden, og håpe at
et skippertak skal ordne biffen.
Det er også viktig å merke seg at læreboka [1] er viktig i dette faget. Det er flere
måter å fremstille algoritmestoff på, og man kan selvfølgelig bruke de ressursene
man vil for å lære seg det. Det finnes mange videoforelesninger og animasjoner og
lignende på nett, og mye er forklart ganske rett frem i Wikipedia og ellers. Men
til syvende og sist så skal det jo være en eksamen i faget, og der må vi ha en eller
annen form for standardisering, av rent praktiske hensyn.
```
```
Derfor baserer vi oss på antagelsen om at dere faktisk leser læreboka, og at
dere leser jevnt gjennom semesteret.
```
```
Forelesningene skal gi dere et første møte med stoffet, slik at det forhåpentligvis
blir lettere for dere å forstå boka når dere setter dere ned for å lese selv. Det er
ingen grunn til panikk om du ikke skjønner absolutt alt i en forelesning, eller om
du ikke husker alt etterpå. Sannsynligvis er du bedre rustet til å jobbe med stoffet
uansett.
```
```
Et hovedtips til studeringen kan være:
```
```
i
```

```
Test deg selv!
```
Det er et velkjent fenomen fra f.eks. forskning på hukommelse at om man leser en
tekst flere ganger så kan man lures til å tro at man husker den, og kanskje til og
med forstår den, bare fordi den virker kjent. Dette er en skummel felle å gå i; det
er lett å tro at man har skjønt mer enn man har! Øvingene gir deg en god mulighet
til å teste hva du faktisk har fått med deg, men det går jo an å finne andre måter å
gjøre det på også.
Ikke bare er det å gjøre oppgaver en viktig måte å teste deg selv på, men det er
også en måte å styrke hukommelsen på – såkalt _recall training_. Det er mulig stoffet
ligger godt lagret, men om du ikke trener på å hente det frem, så kan det være du ikke
husker det likevel. Dessuten er oppgaver en god trening i å anvende ferdighetene
du tilegner deg i faget. Om du er interessert, så finnes det store mengder med
algoritmeløsnings-oppgaver på nett, både knyttet til programmeringskonkurranser
og til jobbintervjuer.
En siste, kanskje litt pussig gevinst ved å løse oppgaver er at det kan bidra til å
bygge opp mentale strukturer som er mottakelige for relaterte ideer. Dette er det
såkalte _pretesting_ -fenomenet: Om du prøver deg på en oppgave fra en del av pensum
du ikke har lært noe om ennå, selv om du _vet_ , at du ikke kommer til å få den til, så
kan det bidra til at du lærer stoffet bedre når du begynner å jobbe med det. Du
trenger ikke se på løsningen til oppgaven engang. Her er det mange muligheter –
det er for eksempel fullt av oppgaver i læreboka, og dem kan det være lurt å kikke
igjennom _før_ du begynner å lere; kanskje til og med før du går til forelesningen.
Dette gjelder gamle eksamensoppgaver også, naturligvis. Vi prøver å variere
form og innhold litt, så man ikke skal kunne «pugge eksamen», men det kan likevel
være lurt å øve seg på gamle eksamener. De nyeste vil trolig være mest relevante.
Her også kan pretesting være en tanke.

```
Hva med å prøve deg på et par eksamenssett allerede den første uka?
```
Og skulle det være noe du ikke forstår, noe du trenger hjelp til eller noe du vil slå
av en prat om, så gjør gjerne det! Om det er spørsmål du tror flere kan ha nytte av
å se svaret på, så spør gjerne i fagets forum på nett. Ellers går det fint an å sende
e-post eller ta personlig kontakt med øvingsstaben eller meg.
Om det er uvant å skulle lære seg relativt teoretiske ting med høy grad av
presisjon, så finnes det generelle råd og tips rundt dette. En bok jeg har anbefalt
flere, og fått gode tilbakemeldinger på, er denne:

```
Bokanbefaling
```
```
A mind for numbers av Barbara Oakley [2]. Nært knyttet til boka er Coursera -
```
```
ii
```

```
kurset Learning how to learn: Powerful tools to help you master tough subjects.
```
```
I de følgende lister med læringsmål er noen merket med utropstegn. Dette er en
ganske uformell greie, men er ment å indikere temaer som kanskje er viktigere enn
man skulle tro. Av og til betyr det de merkede målene er viktigere enn de andre,
men iblant – for mer marginale ting – så er det mer av typen «Obs! Dette bør du
ikke nedprioritere!» Men, som sagt, det er ganske uformelt, og ikke noe du bør legge
altfor stor vekt på.
```
Overordnede læringsmål

```
De overordnede læringsmålene for faget er som følger.∗
Dere skal ha kunnskap om:
```
```
[X 1 ] Et bredt spekter av etablerte algoritmer og datastrukturer
[X 2 ] Klassiske algoritmiske problemer med kjente effektive løsninger
[X 3 ] Komplekse problemer uten kjente effektive løsninger
```
```
Dere skal kunne:
```
```
[X 4 ] Analysere algoritmers korrekthet og effektivitet
[X 5 ] Formulere problemer så de kan løses av algoritmer
```
![X 6 ] Konstruere nye effektive algoritmer

```
Dere skal være i stand til:
```
```
[X 7 ] Å bruke eksisterende algoritmer og programvare på nye problemer
[X 8 ] Å utvikle nye løsninger på praktiske algoritmiske problemstillinger
```
```
Mer spesifikke læringsmål er oppgitt for hver forelesning.
Punkter nedenfor merket meder pensum. Kapittelreferanser er til pensumboka
Introduction to Algorithms [1].
```
Forkunnskaper

```
En viss bakgrunn er nødvendig for å få fullt utbytte av faget, som beskrevet i fagets
anbefalte forkunnskaper.^1 Mye av dette kan man tilegne seg på egen hånd om man
ikke har hatt relevante fag, men det kan da være lurt å gjøre det så tidlig som mulig
i semesteret.
Dere bør:
∗Se emnebeskrivelsen,
https:/ntnu.no/studier/emner/TDT4120.
```
```
iii
```

```
[Y 1 ] Kjenne til begreper rundt monotone funksjoner
[Y 2 ] Kjenne til notasjonened x e,b x c, n !og a mod n
[Y 3 ] Vite hva polynomer er
[Y 4 ] Kjenne grunnleggende potensregning
[Y 5 ] Ha noe kunnskap om grenseverdier
```
![Y 6 ] Være godt kjent med logaritmer med ulike grunntall

```
[Y 7 ] Kjenne enkel sannsynlighetsregning, indikatorvariable og forventning
[Y 8 ] Ha noe kjennskap til rekkesummer (se også side 15 i dette heftet)
[Y 9 ] Beherske helt grunnleggende mengdelære
[Y 10 ]Kjenne grunnleggende terminologi for relasjoner, ordninger og funksjoner
[Y 11 ]Kjenne grunnleggende terminologi for og egenskaper ved grafer og trær
[Y 12 ]Kjenne til enkel kombinatorikk, som permutasjoner og kombinasjoner
```
```
De følgende punktene er ment å dekke de anbefalte forkunnskapene. De er pensum,
men antas i hovedsak kjent (bortsett fra bruken av asymptotisk notasjon). De
vil derfor i liten grad dekkes av forelesningene. De er også i hovedsak ment som
bakgrunnsstoff, til hjelp for å tilegne seg resten av stoffet, og vil dermed i mindre
grad bli spurt om direkte på eksamen.
```
```
Kap. 3. Growth of functions: 3.
```
```
Kap. 5. Probabilistic analysis... : s. 118–
```
```
App. A. Summations: s. 1145–1146 og ligning (A.5)
```
```
App. B. Sets, etc.
```
```
App. C. Counting and probability: s. 1183–1185, C.2 og s. 1196–
```
```
Vi vil generelt prøve å friske opp relevant kunnskap, men om dette stoffet er ukjent,
så regnes det altså stort sett som selvstudium. Dersom det er noe dere finner spesielt
vanskelig, så ta kontakt.
```
```
Vi forventer også at dere behersker grunnleggende programmering, at dere har noe
erfaring med rekursjon og grunnleggende strukturer som tabeller ( arrays ) og lenkede
lister. Dere vil få noe repetisjon av dette.
```
Gjennom semesteret

```
Det følgende er pensum knyttet til flere forelesninger gjennom semesteret:
```
```
Lysark fra ordinære forelesninger
```
```
Appendiks til dette dokumentet
```
```
iv
```

```
Læringsmål for hver algoritme:
```
```
[Z 1 ] Kjenne den formelle definisjonen av det generelle problemet den løser
[Z 2 ] Kjenne til eventuelle tilleggskrav den stiller for å være korrekt
[Z 3 ] Vite hvordan den oppfører seg; kunne utføre algoritmen, trinn for trinn
```
![Z 4 ] Forstå korrekthetsbeviset; hvordan og hvorfor virker algoritmen egentlig?

```
[Z 5 ] Kjenne til eventuelle styrker eller svakheter, sammenlignet med andre
[Z 6 ] Kjenne kjøretidene under ulike omstendigheter, og forstå utregningen
```
```
Læringsmål for hver datastruktur:
```
```
[Z 7 ] Forstå algoritmene (jf. målZ 01 – Z 06 ) for de ulike operasjonene på strukturen
[Z 8 ] Forstå hvordan strukturen representeres i minnet
```
```
Læringsmål for hvert problem:
```
```
[Z 9 ] Kunne angi presist hva input er
[Z 10 ]Kunne angi presist hva output er og hvilke egenskaper det må ha
```
```
v
```

Pensumoversikt

Delkapitler er også listet opp under tilhørende forelesning, sammen med læringsmål.
Kapittel- og oppgavereferanser er til _Introduction to Algorithms_ av Cormen mfl. [1].

Kap. 1. The role of algorithms in computing

Kap. 2. Getting started

Kap. 3. Growth of functions: Innledning og 3.

Kap. 4. Divide-and-conquer: Innledning, 4.1 og 4.3–4.

Kap. 6. Heapsort

Kap. 7. Quicksort

Kap. 8. Sorting in linear time

Kap. 9. Medians and order statistics

Kap. 10. Elementary data structures

Kap. 11. Hash tables: s. 253–

Kap. 12. Binary search trees: Innledning og 12.1–12.

Kap. 15. Dynamic programming: Innledning og 15.1, 15.3–15.

Kap. 16. Greedy algorithms: Innledning og 16.1–16.

Kap. 17. Amortized analysis: Innledning og s. 463–465 (tom. «at most 3»)

Kap. 21. Data structures for disjoint sets: Innledning, 21.1 og 21.

Kap. 22. Elementary graph algorithms: Innledning og 22.1–22.

Kap. 23. Minimum spanning trees

Kap. 24. Single-source shortest paths: Innledning og 24.1–24.

Kap. 25. All-pairs shortest paths: Innledning, 25.2 og 25.

Kap. 26. Maximum flow: Innledning og 26.1–26.

Kap. 34. NP-completeness

Oppgaver 2.3-5, 4.5-3, 16.2-2 og 34.1-4, med løsning

Appendiks A–F i dette heftet


## Forelesning 1

# Problemer og algoritmer

```
Vi starter med fagfeltets grunnleggende byggesteiner, og skisserer et rammeverk for
å tilegne seg resten av stoffet. Spesielt viktig er ideen bak induksjon og rekursjon:
Vi trenger bare se på siste trinn , og kan anta at resten er på plass.
```
```
Pensum:
```
```
Kap. 1. The role of algorithms in computing
```
```
Kap. 2. Getting started: Innledning, 2.1–2.
```
```
Kap. 3. Growth of functions: Innledning og 3.
```
```
Læringsmål:
```
```
[A 1 ] Forstå bokas pseudokode -konvensjoner
[A 2 ] Kjenne egenskapene til random-access machine -modellen (RAM)
[A 3 ] Kunne definere problem , instans og problemstørrelse
```
![A 4 ] Kunne definere _asymptotisk notasjon_ ,O,Ω,Θ, _o_ og _ω_.

![A 5 ] Kunne definere _best-case_ , _average-case_ og _worst-case_

![A 6 ] Forstå _løkkeinvarianter_ og _induksjon_

![A 7 ] Forstå _rekursiv dekomponering_ og _induksjon over delproblemer_

```
[A 8 ] ForståInsertion-Sort
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 2

# Datastrukturer

```
For å unngå grunnleggende kjøretidsfeller er det viktig å kunne organisere og struk-
turere data fornuftig. Her skal vi se på hvordan enkle strukturer kan implementeres
i praksis, og hva vi vinner på å bruke dem i algoritmene våre.
```
```
Pensum:
```
```
Kap. 10. Elementary data structures: Innledning og 10.1–10.
```
```
Kap. 11. Hash tables: s. 253–
```
```
Kap. 17. Amortized analysis: Innledning og s. 463–465 (tom. «at most 3»)
```
```
Læringsmål:
```
```
[B 1 ] Forstå hvordan stakker og køer fungerer
(Stack-Empty,Push,Pop,Enqueue,Dequeue)
[B 2 ] Forstå hvordan lenkede lister fungerer
(List-Search,List-Insert,List-Delete,List-Delete′,List-Search′,List-Insert′)
[B 3 ] Forstå hvordan pekere og objekter kan implementeres
```
![B 4 ] Forstå hvordan _direkte adressering_ og _hashtabeller_ fungerer

```
(Hash-Insert,Hash-Search)
[B 5 ] Forstå konfliktløsing ved kjeding ( chaining )
(Chained-Hash-Insert,Chained-Hash-Search,Chained-Hash-Delete)
[B 6 ] Kjenne til grunnleggende hashfunksjoner
[B 7 ] Vite at man for statiske datasett kan ha worst-case O(1)for søk
[B 8 ] Kunne definere amortisert analyse
[B 9 ] Forstå hvordan dynamiske tabeller fungerer
(Table-Insert)
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 3

# Splitt og hersk

```
Rekursiv dekomponering er kanskje den viktigste ideen i hele faget, og designmeto-
den splitt og hersk er en grunnleggende utgave av det: Del instansen i mindre biter,
løs problemet rekursivt for disse, og kombinér løsningene.
```
```
Pensum:
```
```
Kap. 2. Getting started: 2.
```
```
Kap. 4. Divide-and-conquer: Innledning, 4.1 og 4.3–4.
```
```
Kap. 7. Quicksort
```
```
Oppgaver 2.3-5 og 4.5-3 med løsning (binærsøk)
```
```
Appendiks B og C i dette heftet
```
```
Læringsmål:
```
![C 1 ] Forstå designmetoden _divide-and-conquer_ ( _splitt og hersk_ )

```
[C 2 ] Forstå maximum-subarray -problemet med løsninger
[C 3 ] ForståBisectogBisect′(se appendiks C i dette heftet)
[C 4 ] ForståMerge-Sort
[C 5 ] ForståQuicksortogRandomized-Quicksort
```
![C 6 ] Kunne løse rekurrenser med _substitusjon_ , _rekursjonstrær_ og _masterteoremet_

![C 7 ] Kunne løse rekurrenser med _iterasjonsmetoden_ (se appendiks B i dette heftet)

```
[C 8 ] Forstå hvordan variabelskifte fungerer
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 4

# Rangering i lineær tid

```
Vi kan ofte få bedre løsninger ved å styrke kravene til input eller ved å svekke
kravene til output. Sortering basert på sammenligninger ( x 6 y ) er et klassisk
eksempel: I verste tilfelle må vi brukelg n !sammenligninger,men om vi antar mer
om elementene eller bare sorterer noen av dem så kan vi gjøre det bedre.
```
```
Pensum:
```
```
Kap. 8. Sorting in linear time
```
```
Kap. 9. Medians and order statistics
```
```
Læringsmål:
```
![D 1 ] Forstå hvorfor _sammenligningsbasert sortering_ har en _worst-case_ påΩ( _n_ lg _n_ )

```
[D 2 ] Vite hva en stabil sorteringsalgoritme er
[D 3 ] ForståCounting-Sort, og hvorfor den er stabil
```
![D 4 ] ForståRadix-Sort, og hvorfor den trenger en stabil subrutine

```
[D 5 ] ForståBucket-Sort
[D 6 ] ForståRandomized-Select
[D 7 ] Kjenne tilSelect
```
```
Merk: Det kreves ikke grundig forståelse av virkemåten tilSelect.
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 5

# Rotfaste trestrukturer

```
Rotfaste trær gjenspeiler rekursiv dekomponering. I binære søketrær er alt i venst-
re deltre mindre enn rota, mens alt i høyre deltre er større, og det gjelder rekursivt
for alle deltrær! Hauger er enklere: Alt er mindre enn rota. Det begrenser funksjo-
naliteten, men gjør dem billigere å bygge og balansere.
```
```
Pensum:
```
```
Kap. 6. Heapsort
```
```
Kap. 10. Elementary data structures: 10.
```
```
Kap. 12. Binary search trees: Innledning og 12.1–12.
```
```
Læringsmål:
```
![E 1 ] Forstå hvordan _heaps_ fungerer, og hvordan de kan brukes som _prioritetskøer_

```
(Parent,Left,Right,Max-Heapify,Build-Max-Heap,Heapsort,Max-Heap-Insert,
Heap-Extract-Max,Heap-Increase-Key,Heap-Maximum. Også tilsvarende for min-
heaps , f.eks.,Build-Min-HeapogHeap-Extract-Min.)
[E 2 ] ForståHeapsort
[E 3 ] Forstå hvordan rotfaste trær kan implementeres
```
![E 4 ] Forstå hvordan _binære søketrær_ fungerer

```
(Inorder-Tree-Walk,Tree-Search,Iterative-Tree-Search,Tree-Minimum,Tree-
Maximum,Tree-Successor,Tree-Predecessor,Tree-Insert,Transplant,Tree-Delete)
[E 5 ] Vite at forventet høyde for et tilfeldig binært søketre erΘ(lg n )
[E 6 ] Vite at det finnes søketrær med garantert høyde påΘ(lg n )
```
```
Merk: Det kreves ikke grundig forståelse avTransplantogTree-Delete.
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 6

# Dynamisk programmering

```
På sett og vis en generalisering av splitt og hersk , der delprobler kan overlappe:
I stedet for et tre av delproblem-avhengigheter har vi en rettet asyklisk graf. Vi
finner og lagrer del-løsninger i en rekkefølge som stemmer med avhengighetene.
```
```
Pensum:
```
```
Kap. 15. Dynamic programming: Innledning og 15.1, 15.3–15.
```
```
Oppgave 16.2-2 med løsning (0-1 knapsack)
```
```
Appendiks D i dette heftet
```
```
Delkapitler 15.2 og 15.5 kan være nyttige som støttelitteratur.
Læringsmål:
```
![F 1 ] Forstå ideen om en _delproblemgraf_

![F 2 ] Forstå designmetoden _dynamisk programmering_

![F 3 ] Forstå løsning ved _memoisering_ ( _top-down_ )

```
[F 4 ] Forstå løsning ved iterasjon ( bottom-up )
[F 5 ] Forstå hvordan man rekonstruerer en løsning fra lagrede beslutninger
[F 6 ] Forstå hva optimal delstruktur er
[F 7 ] Forstå hva overlappende delproblemer er
[F 8 ] Forstå eksemplene stavkutting og LCS
[F 9 ] Forstå løsningen på 0-1-ryggsekkproblemet (se appendiks D i dette heftet)
(Knapsack,Knapsack′)
```
```
Merk: Noe av stoffet vil kanskje forskyves til forelesning 7.
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 7

# Grådige algoritmer

```
Grådige algoritmer består av en serie med valg, og hvert valg tas lokalt : Algorit-
men gjør alltid det som ser best ut her og nå, uten noe større perspektiv. Slike
algoritmer er ofte enkle; utfordringen ligger i å finne ut om de gir rett svar.
```
```
Pensum:
```
```
Kap. 16. Greedy algorithms: Innledning og 16.1–16.
```
```
Læringsmål:
```
![G 1 ] Forstå designmetoden _grådighet_

![G 2 ] Forstå _grådighetsegenskapen_ ( _the greedy-choice property_ )

```
[G 3 ] Forstå eksemplene aktivitet-utvelgelse og det fraksjonelle ryggsekkproblemet
[G 4 ] ForståHuffmanog Huffman-koder
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 8

# Traversering av grafer

```
Vi traverserer en graf ved å besøke noder vi vet om. Vi vet i utgangspunktet bare
om startnoden, men oppdager naboene til dem vi besøker. Traversering er viktig i
seg selv, men danner også ryggraden til flere mer avanserte algoritmer.
```
```
Pensum:
```
```
Kap. 22. Elementary graph algorithms: Innledning og 22.1–22.
```
```
Appendiks E i dette heftet
```
```
Læringsmål:
```
```
[H 1 ] Forstå hvordan grafer kan implementeres
[H 2 ] ForståBFS, også for å finne korteste vei uten vekter
[H 3 ] ForståDFSog parentesteoremet
[H 4 ] Forstå hvordanDFS klassifiserer kanter
[H 5 ] ForståTopological-Sort
[H 6 ] Forstå hvordanDFSkan implementeres med en stakk
[H 7 ] Forstå hva traverseringstrær (som bredde-først - og dybde-først -trær) er
```
![H 8 ] Forstå _traversering med vilkårlig prioritetskø_

```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 9

# Minimale spenntrær

```
Her har vi en graf med vekter på kantene, og ønsker å bare beholde akkurat de
kantene vi må for å koble sammen alle nodene, med en så lav vektsum som mulig.
Erke-eksempel på grådighet: Velg én og én kant, alltid den billigste lovlige.
```
```
Pensum:
```
```
Kap. 21. Data structures for disjoint sets: Innledning, 21.1 og 21.
```
```
Kap. 23. Minimum spanning trees
```
```
Læringsmål:
```
```
[I 1 ] Forstå skog -implementasjonen av disjunkte mengder
(Connected-Components,Same-Component,Make-Set,Union,Link,Find-Set)
[I 2 ] Vite hva spenntrær og minimale spenntrær er
```
![I 3 ] ForståGeneric-MST

```
[I 4 ] Forstå hvorfor lette kanter er trygge kanter
[I 5 ] ForståMST-Kruskal
[I 6 ] ForståMST-Prim
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 10

# Korteste vei fra én til alle

```
Bredde-først-søk kan finne stier med færrest mulig kanter, men hva om kantene
har ulik lengde? Det generelle problemet er uløst, men vi kan løse problemet med
gradvis bedre kjøretid for grafer (1) uten negative sykler; (2) uten negative kanter;
og (3) uten sykler. Og vi bruker samme prinsipp for alle tre!
```
```
Pensum:
```
```
Kap. 24. Single-source shortest paths: Innledning og 24.1–24.
```
```
Læringsmål:
```
```
[J 1 ] Forstå ulike varianter av korteste-vei - eller korteste-sti -problemet
( Single-source , single-destination , single-pair , all-pairs )
[J 2 ] Forstå strukturen til korteste-vei -problemet
[J 3 ] Forstå at negative sykler gir mening for korteste enkle vei ( simple path )
[J 4 ] Forstå at korteste enkle vei kan løses vha. lengste enkle vei og omvendt
[J 5 ] Forstå hvordan man kan representere et korteste-vei-tre
```
![J 6 ] Forstå _kant-slakking_ ( _edge relaxation_ ) ogRelax

```
[J 7 ] Forstå ulike egenskaper ved korteste veier og slakking
( Triangle inequality , upper-bound property , no-path property , convergence property , path-
relaxation property , predecessor-subgraph property )
[J 8 ] ForståBellman-Ford
[J 9 ] ForståDAG-Shortest-Path
```
![J 10 ] Forstå kobling mellomDAG-Shortest-Pathog dynamiskprogrammering

```
[J 11 ] ForståDijkstra
```
```
Merk: Noe av stoffet vil kanskje forskyves til forelesning 11.
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 11

# Korteste vei fra alle til alle

```
Vi kan finne de korteste veiene fra hver node etter tur, men mange av delproble-
mene vil overlappe – om vi har mange nok kanter lønner det seg å bruke dynamisk
programmering med dekomponeringen «Skal vi innom k eller ikke?»
```
Pensum:

Kap. 25. All-pairs shortest paths: Innledning, 25.2 og 25.

Læringsmål:

[K 1 ] Forstå _forgjengerstrukturen_ for _alle-til-alle_ -varianten av korteste vei-problemet

```
(Print-All-Pairs-Shortest-Path)
```
[K 2 ] ForståFloyd-Warshall

[K 3 ] ForståTransitive-Closure

[K 4 ] ForståJohnson

Merk: Noe stoff fra forelesning 12 vil kanskje dekkes her.

Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.


## Forelesning 12

# Maksimal flyt

```
Et stort skritt i retning av generell lineær optimering (såkalt lineær programme-
ring). Her ser vi på to tilsynelatende forskjellige problemer, som viser seg å være
duale av hverandre, noe som hjelper oss med å finne en løsning.
```
```
Pensum:
```
```
Kap. 26. Maximum flow: Innledning og 26.1–26.
```
```
Læringsmål:
```
```
[L 1 ] Kunne definere flytnett , flyt og maks-flyt-problemet
[L 2 ] Kunne håndtere antiparallelle kanter og flere kilder og sluk
```
![L 3 ] Kunne definere _restnettet_ til et flytnett med en gitt flyt

```
[L 4 ] Forstå hvordan man kan oppheve ( cancel ) flyt
[L 5 ] Forstå hva en forøkende sti ( augmenting path ) er
[L 6 ] Forstå hva snitt , snitt-kapasitet og minimalt snitt er
```
![L 7 ] Forstå _maks-flyt/min-snitt-teoremet_

```
[L 8 ] ForståFord-Fulkerson-MethodogFord-Fulkerson
[L 9 ] Vite atFord-FulkersonmedBFSkalles Edmonds-Karp -algoritmen
[L 10 ] Forstå hvordan maks-flyt kan finne en maksimum bipartitt matching
```
![L 11 ] Forstå _heltallsteoremet_ ( _integrality theorem_ )

```
Merk: Noe av stoffet vil kanskje dekkes i forelesning 11.
```
```
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.
```

## Forelesning 13

# NP-kompletthet

```
NP er den enorme klassen av ja-nei -problemer der ethvert ja-svar har et bevis som
kan sjekkes i polynomisk tid. Alle problemer i NP kan i polynomisk tid reduseres
til de såkalt komplette problemene i NP. Dermed kan ikke disse løses i polynomisk
tid, med mindre alt i NP kan det. Ingen har klart det så langt...
```
```
Pensum:
```
```
Kap. 34. NP-completeness
```
```
Oppgave 34.1-4 med løsning (0-1 knapsack)
```
```
Se også appendiks D i dette heftet.
Læringsmål:
```
```
[M 1 ] Forstå sammenhengen mellom optimerings- og beslutnings-problemer
[M 2 ] Forstå koding ( encoding ) av en instans
[M 3 ] Forstå hvorfor løsningen vår på 0-1-ryggsekkproblemet ikke er polynomisk
[M 4 ] Forstå forskjellen på konkrete og abstrakte problemer
[M 5 ] Forstå representasjonen av beslutningsproblemer som formelle språk
[M 6 ] Forstå definisjonen av klassene P, NP og co-NP
```
[M 7 ] Forstå _redusibilitets-relasjonen_ (^6) P
![M 8 ] Forstå definisjonen av NP _-hardhet_ og NP _-kompletthet_
[M 9 ] Forstå den konvensjonelle hypotesen om forholdet mellom P, NP og NPC
![M 10 ]Forstå hvordan NP-kompletthet kan bevises ved én reduksjon
![M 11 ]Kjenne de NP-komplette problemene CIRCUIT-SAT, SAT, 3-CNF-SAT,
CLIQUE, VERTEX-COVER, HAM-CYCLE, TSP og SUBSET-SUM
[M 12 ]Forstå at _0-1-ryggsekkproblemet_ er NP-hardt
[M 13 ]Forstå at _lengste enkle-vei_ -problemet er NP-hardt
[M 14 ]Være i stand til å konstruere enkle NP-kompletthetsbevis
Merk: Det kreves ikke grundig forståelse av de ulike NP-kompletthetsbevisene.
Husk også læringsmålZ 1 – Z 10 på side iv, som gjelder hver forelesning.


## Appendiks A

# Noen gjengangere

Enkelte tema dukker opp flere ganger i faget, uten at de nødvendigvis er knyttet
til én bestemt forelesning eller ett bestemt pensumkapittel. Her beskriver jeg noen
slike tema som det kan være greit å få litt i fingrene fra tidlig av, siden det kan
gjøre det lettere å forstå en god del andre ting i faget.

**Håndtrykksformelen**

Dette er én av to enkle men viktige formler som stadig dukker opp når man skal
analysere algoritmer. Formelen er:

```
n ∑− 1
```
```
i =0
```
```
i =
```
```
n ( n −1)
2
```
Den kalles gjerne _håndtrykksformelen_ , fordi den beskriver antall håndtrykk som
utføres om _n_ personer skal hilse på hverandre. Vi kan vise dette ved å telle antall
håndtrykk på to ulike måter. De to resultatene må da være like.

Telling 1:La oss se på personene én etter én. Første person hilser på alle de andre
_n_ − 1 personene. Andre person har allerede hilst på første person, men bidrar med
_n_ − 2 nye håndtrykk ved å hilse på de gjenværende. Generelt vil person _i_ bidra med
_n_ − _i_ nye håndtrykk, så totalen blir

```
( n −1) + ( n −2) +···+ 2 + 1 + 0.
```
Om vi snur summen, så vi får0 + 1 +···+ ( _n_ −1), så er dette den venstre siden av
ligningen. Dette oppstår typisk i algoritmer de vi utfører en løkke gjentatte ganger,
og antall iterasjoner i løkken øker eller synker med 1 for hver gang, slik:

1 for _i_ = 1to _n_ − 1
2 for _j_ = _i_ + 1to _n_
3 _i_ tar _j_ i hånden

Telling 2:Vi kan også telle på en mer rett frem måte: Hver person tar alle de andre
i hånden, og inngår dermed i _n_ − 1 håndtrykk. Hvis vi bare teller hver enkelt person
sin halvdel av håndtrykket, får vi altså _n_ ( _n_ −1) _halve_ håndtrykk. To slike halve
håndtrykk utgjør jo ett helt, så det totale antallet håndtrykk blir den høyre siden
av ligningen, nemlig _n_ ( _n_ −1) _/_ 2.


```
Man kan også gjøre om en sum av denne typen ved å brette den på midten, og
legge sammen første og siste element, nest første og nest siste, etc. Vi får da
```
```
( n −1 + 0) + ( n −2 + 1) + ( n −3 + 2) +...
```
```
Hvert ledd summerer til n − 1 , og det er n/ 2 ledd. Mer generelt er summen av en
aritmetisk rekke (der vi øker med en konstant fra ledd til ledd) lik gjennomsnittet
av første og siste ledd, multiplisert med antallet ledd i rekken.
```
Utslagsturneringer

```
Dette er den andre av de to sentrale formlene:
h ∑− 1
```
```
i =0
```
```
2 i = 2 h − 1
```
```
Det vil si, de første toerpotensene summerer til én mindre enn den neste. En
måte å se dette på er av totallssystemet, der et tall a = 11··· 1 med h ettall
etterfølges av tallet b = 100··· 0 , som består av ett ettall, og h nuller. Her er
a = 2^0 + 2^1 +···+ 2 h −^1 og b = 2 h , så a = b − 1.
Et grundigere bevis kan vi få ved å bruke samme teknikk som før, og telle
samme ting på to ulike måter. Det vi vil telle er antall matcher i en utslagsturnering
( knockout -turnering), det vil si, en turnering der taperen i en match er ute av spillet.
Dette blir altså annerledes enn såkalte round robin -turneringer, der alle møter alle
```
- for dem kan vi bruke håndtrykksformelen for å finne antall matcher, siden hver
    match tilsvarer ett håndtrykk.

```
Telling 1:Vi begynner med å sette opp et turneringstre :
```
```
1 2 3 4 ··· n
```
```
h
```
```
Her er deltakerne plassert nederst, i løvnodene; hver av de tomme, hvite nodene
representerer en match, og vil etterhvert fylles med vinneren av de to i nodene
under, helt til vi står igjen med én vinner på toppen. Vi går altså gjennom h
runder, og i hver runde organiseres de spillerne som gjenstår i par som skal møtes.
Om vi nummererer rundene baklengs, etter hvor mange runder det er igjen til
finalen, så vil antall matcher i runde i være 2 i. Det totale antall matcher er altså
20 + 2^1 +···+ 2 h −^1 , som er venstresiden av ligningen vår.
```

```
Telling 2:Antall deltakere er n = 2 h. I hver match blir én deltaker slått ut, helt til
vi sitter igjen med én vinner etter finalen. Antall matcher trenger vi for å slå ut
alle bortsett fra én er n − 1 , eller 2 h − 1 , som er høyresiden i ligningen.
```
```
Det er forøvrig også viktig å merke seg høyden til treet : h =log 2 n. Dette er altså
antall doblinger fra 1 til n , eller antall halveringer fra n til 1. Det er også en
sammenheng som vil dukke opp ofte.
```
Reduksjoner

```
Ofte jobber vi med å bryte ned problemer i sine enkelte bestanddeler, og deretter
bygge opp en løsning fra bunnen av, trinn for trinn. Men av og til har vi kanskje
allerede en løsning på et problem som ligner på det vi prøver å løse. Eller kanskje vi
vet at et lignende problem er spesielt vanskelig å løse – at ingen har klart det ennå?
I slike sammenhenger kan vi benytte oss av det som kalles en reduksjonsalgoritme ,
eller reduksjon. En reduksjon transformerer input fra ett problem til et annet, slik
at vi kan konsentrere oss om å løse problemet vi har redusert til. Transformasjonen
må være slik at om vi løser det nye problemet, så vil løsningen også være riktig
for det opprinnelige problemet (gitt opprinnelig input). Av og til tillater vi oss å
transformere svaret tilbake, og regner denne transformasjonen også som en del av
reduksjonen.
```
```
input til A
```
```
input til B output fra B
```
```
output fra A
```
```
reduksjon
```
```
evt. løsning på B
```
```
Ut fra dette kan vi trekke to logisk ekvivalente konklusjoner:
```
( _i_ ) Hvis vi kan løse B, så kan vi løse A

( _ii_ ) Hvis vi _ikke_ kan løse A, så kan vi _ikke_ løse B

```
Det første utsagnet ser vi av figuren over. Straks vi plugger inn en løsning for B,
så kan den kombineres med reduksjonen for å lage en løsning for A. Det andre
utsagnet følger ved kontraposisjon. Om vi lar LA og LB være de logiske utsagnene
at vi har en løsning for hhv. A og B, så kan vi skrive om konklusjonene:
```
```
LB ⇒ LA og ¬LA ⇒ ¬LB
```
Vi kan også slenge på et par betraktningene:

( _iii_ ) Hvis vi _ikke_ kan løse B, så _sier det ingenting_ om A

( _iv_ ) Hvis vi _kan_ løse A, så _sier det ingenting_ om B


Selv om vi _ikke_ kan løse B, så kan det jo hende at vi kan løse A på en _annen_ måte.
Og om vi _kan_ løse A, så er det jo ikke sikkert at vi gjorde det ved å redusere til
B. Disse tilfellene gir oss altså ingen informasjon. Med andre ord: Om vi allerede
er kjent med et problem X og så støter på et nytt og ukjent problem Y, så har vi
to scenarier der vi kan gjøre noe fornuftig. Hva vi gjør avhenger av om vi lar Y få
rollen som A eller B i figuren over. Hvis vi vi vise at Y _ikke er vanskeligere enn_ X,
så kan vi la Y innta rollen som A, og prøve å finne en reduksjon fra Y til X. Det er
dette vi ofte gjør når vi prøver å bruke eksisterende algoritmer for et problem X å
løse et nytt problem Y. Vi reduserer Y til X, og løser så X.
Men av og til mistenker vi at et problem vi støter på er vanskelig. Kanskje vi
kjenner til et problem X som vi _vet_ er vanskelig, og vi vil vise at Y er _minst like
vanskelig_. Da må vi i stedet la Y innta rollen som B, og redusere _fra_ det vanskelige
problemet. Vi skriverA 6 Bfor å uttrykke at problemet A er _redusibelt_ til B, det
vil si, at det finnes en reduksjon fra A til B, så A kan _løses ved hjelp av_ B. Det betyr
altså at A ikke er vanskeligere enn B, siden vi jo kan redusere til B; og ekvivalent,
at B er minst like vanskelig som A.
Vanligvis ønsker vi å være noe mer nyanserte enn at det finnes eller ikke finnes
en løsning på et problem. Vi kan for eksempel lure på hvor effektive algoritmer vi
kan finne for problemet. Da er det viktig at også _reduksjonen_ vår er effektiv. For
dersom reduksjonen bruker ubegrenset lang tid, så vil jo ikke en effektiv løsning på
B fortelle oss noe om hvor effektivt vi kan løse A.


## Appendiks B

# Iterasjonsmetoden

I læreboka er _rekursjonstrær_ en sentral metode for å løse rekurrensligninger. Dette
er en mer visuell fremgangsmåte enn den som ble brukt i tidligere utgaver, som
de kalte _iterasjonsmetoden_. Det er egentlig akkurat samme prinsipp, bare at man
jobber mer direkte på ligningene. Ideen er at man bruker rekurrensen selv til å
ekspandere de rekursive termene. For eksempel:

```
T(0) = 0
T( n ) = T( n −1) + 1 ( n >1)
```
Her ønsker vi å bli kvittT( _n_ −1)på høyre side i ligningen. Om vi antar at _n >_ 1 ,
så kan vi bruke definisjonenT( _n_ ) = T( _n_ −1) + 1til å finne ut hvaT( _n_ −1)er,
nemlig(T(( _n_ −1)−1) + 1) + 1ellerT( _n_ −2) + 2. Det er altså én _ekspansjon_ (eller
iterasjon). Vi kan fortsette på samme måte, trinn for trinn, helt til vi ser et mønster
dukke opp:

```
T(0) = 0
T( n ) = T( n −1) + 1 (1)
= T( n −2) + 2 (2)
= T( n −3) + 3 (3)
..
.
```
### ..

### .

```
= T( n − i ) + i ( i )
= T( n − n ) + n = n ( n )
```
Først ekspanderer vi altsåT( _n_ ), såT( _n_ −1)og såT( _n_ −2), og så videre, hele tiden
ved hjelp av den opprinnelige rekurrensen. Linjenummeret (til høyre) viser hvor
mange ekspansjoner vi har gjort. Etter hvert klarer vi altså å uttrykke resultatet
etter et vilkårlig antall ekspansjoner, _i_. Her kan _i_ være 1, 2, 3, eller et hvilket som
helst annet tall, så lenge vi ikke ekspanderer oss «forbi» grunntilfelletT(0) = 0.
Og det er nettopp grunntilfellet vi ønsker å nå; vi vil giTargumentet 0. For å
få til det må vi altså få _n_ − _i_ til å bli 0 , og altså sette _i_ = _n_. Dette gir oss svaret,
som vist i siste linje. Et litt mer interessant eksempel finner du i appendiks C, der
hver ekspansjon _halverer_ argumentet tilT, så trenger barelg _n_ ekspansjoner før vi
kommer ned tilT(1), som er grunntilfellet der.


## Appendiks C

# Binærsøk

```
Læreboka sin beskrivelse av binærsøk, på engelsk binary search eller bisection ,
begrenser seg til et par oppgaver (2.3-5 og 4.5-3, med problemet definert i 2.1-
3). Algoritmen er gjennomgått i mer detalj i forelesningene, men den sentrale
informasjonen er oppsummert her. Selve algoritmen er definert som følger, der
tabellenA[1.. n ]antas å være sortert.
```
```
Bisect(A , p, r, v )
1 if p 6 r
2 q =b( p + r ) / 2 c
3 if v ==A[ q ]
4 return q
5 else if v < A[ q ]
6 returnBisect(A , p, q − 1 , v )
7 else returnBisect(A , q + 1 , r, v )
8 else returnnil
```
DersomAinneholder _v_ , så vilBisect(A _,_ 1 _, n, v_ )returnere en indeks _i_ som er slik
atA[ _i_ ]== _v_. Hvis ikke, så returneresnil.
Korrekthet kan vises ved induksjon over lengden til segmentetA[ _p.. r_ ]. Vi har
to grunntilfeller: Enten er _p > r_ , så segmentet er tomt, og vi returnerernil, ellers
så har vi _p_ = _r_ og _v_ = A[ _q_ ], og vi returnerer _q_. Begge disse er korrekte. Anta så
at _p_ 6 _r_ , og atBisectgir rett svar for kortere intervaller. BådeA[ _p.. q_ −1]og
A[ _q_ + 1_.. r_ ]er kortere intervaller, så samme hvilket rekursivt kall som velges, så vil
vi returnere rett indeks.
For å finne kjøretiden kan vi for eksempel telle antall ganger sammenligningen
_p_ 6 _r_ utføres. Vi kan gjøre noen antagelser for å forenkle utregningen. Disse har
ingenting å si for den asymptotiske kjøretiden. For det første antar vi at _n_ = 2 _k_ ,
for et eller annet heltall _k_ > 0 , slik at vi alltid kan dele nøyaktig på midten, helt
til vi ender med _p_ == _q_. For det andre så antar vi at _v_ finnes iAså rekursjonen
stanser her. For å sikre at vi deler på midten, kan vi til og med anta at den eneste
forekomsten av _v_ erA[ _n_ ], så vi alltid velger det høyre segmentet,A[ _q_ + 1_.. r_ ]. (Vi
kan bruke induksjon, dvs., _substitusjonsmetoden_ , til å verifisere at den asymptotiske
kjøretiden vi finner er gyldig også om disse antagelsene _ikke_ gjelder.) Vi får da


følgende rekurrensutregning.

```
T(1) = 1
T( n ) = T( n/ 2) + 1 (1)
= T( n/ 4) + 2 (2)
= T( n/ 8) + 3 (3)
..
.
```
### ..

### .

```
= T( n/ 2 i ) + i ( i )
= T( n/n ) + lg n = lg n + 1 (lg n )
```
Med andre ord er kjøretidenΘ(lg _n_ ).
Som så ofte ellers, så hjelper det å tenke rekursivt for å forstå hvordan algoritmen
fungerer, men det er, som alltid, mulig å kvitte seg med rekursjonen og få en iterativ
versjon:

Bisect′(A _, p, r, v_ )
1 while _p_ 6 _r_
2 _q_ =b( _p_ + _r_ ) _/_ 2 c
3 if _v_ ==A[ _q_ ]
4 return _q_
5 else if _v <_ A[ _q_ ]
6 _r_ = _q_ − 1
7 else _p_ = _q_ + 1
8 returnnil

Denne versjonen vil generelt være mer effektiv, siden man slipper ekstra kostnader
ved funksjonskall.∗Både korrekthetsbevis og kjøretidsberegning kan oversettes
ganske direkte fraBisecttilBisect′.

∗Siden returverdien fra det rekursive kallet returneres direkte, uten noe mer kode imellom – et
såkalt _tail call_ – så vil enkelte språk kunne gjøre denne transformasjonen automatisk, ved hjelp
av såkalt _tail-call elimination_ eller _tail-call optimization_.


## Appendiks-D

# Ryggsekkproblemet

```
Det såkalte ryggsekkproblemet kommer i flere varianter, og to av dem er beskrevet
på side 425–426 i læreboka. Den fraksjonelle varianten er lett å løse: Man tar bare
med seg så mye som mulig av den dyreste gjenstanden, og fortsetter nedover på
lista, sortert etter kilopris. I 0-1-varianten, derimot, blir ting litt vanskeligere—her
må man ta med en hel gjenstand eller la den ligge. Løsningen er egentlig beskrevet
på 426, men den er litt bortgjemt i teksten, og er beskrevet svært skissepreget.
Akkurat som i for eksempelFloyd-Warshallså baserer dekomponeringen
seg på et ja-nei -spørsmål, i dette tilfelle «Skal vi ta med gjenstand i ?» For hver av
de to mulighetene sitter vi igjen med et delproblem som vi løser rekursivt (i hvert
fall konseptuelt). Som vanlig tenker vi oss at dette er siste trinn, og antar at vi har
gjenstander 1 ,... , i tilgjengelige. Vi har da følgende alternativer:
```
( _i_ ) Ja, vi tar med gjenstand _i_. Vi løser så problemet for gjenstander 1 _,... , i_ − 1
men der kapasiteten er redusert med _wi_. Vi legger så til _vi_ til slutt.

( _ii_ ) Nei, vi tar ikke med gjenstand _i_. Vi løser så problemet for gjenstander 1 _,... , i_ −
1 , men kan fortsatt bruke hele kapasiteten. Til gjengjeld får vi ikke legge til _vi_
til slutt.

```
Situasjonen er illustrert i Fig. D.1, der hver rute representerer en delløsning (en
celle i løsningstabellen, for eksempel) og pilene er avhengigheter, som vanlig. Vi kan
sette opp en rekursiv løsning slik (der vi antar at vektene og verdiene er globale
variable, for enkelhets skyld):
```
```
inkludér obj. n ekskludér obj. n
```
```
n
```
```
W
```
```
W− wn
n − 1 n − 1
```
```
+ vn W
```
```
Figur D.1: Dekomponering for 0-1-knapsack.
```

```
Knapsack( n, W)
1 if n == 0
2 return 0
3 x =Knapsack( n − 1 , W)
4 if wn> W
5 return x
6 else y =Knapsack( n − 1 , W− wn ) + vn
7 returnmax( x, y )
```
```
Denne prosedyren vil naturligvis ha eksponentiell kjøretid. Vi kan løse det ved å
bruke memoisering, eller vi kan skrive den om til en iterativ bottom-up -løsning, som
nedenfor.
Knapsack′( n, W)
1 letK[0.. n, 0.. W]be a new table
2 for j = 0toW
3 K[0 , j ] = 0
4 for i = 1to n
5 for j = 0toW
6 x = K[ i − 1 , j ]
7 if j < wi
8 K[ i, j ] = x
9 else y = K[ i − 1 , j − wi ] + vi
10 K[ i, j ] = max( x, y )
```
```
Forskjellen ligger i at vi her eksplisitt legger innfor-løkker for å gå gjennom alle
delproblemene, heller enn å gjøre det direkte (implisitt) med rekursjon.
```
**Men dette er ikke polynomisk!**

Det binære ryggsekkproblemet ( _0-1-knapsack_ ) er et såkalt NP-hardt problem, og
det er ingen som har funnet noen polynomisk løsning på det; trolig vil ingen noen
gang finne det heller. Det ser jo ut til å motsi diskusjonen over, der vi kom frem til
det som absolutt _ser ut som_ en algoritme med polynomisk kjøretid. Hva er det som
foregår her?
Kjøretiden til bådeKnapsackogKnapsack′erΘ( _n_ W), siden det er _n_ W
delproblemer, og vi utfører en konstant mengde arbeid per delproblem. Og det er
rett at dette er en polynomisk funksjon av _n_ ogW, men det er ikke nok til at vi
kan si at algoritmene «har polynomisk kjøretid». Spørsmålet er _som funksjon av
hva_? Hvis vi eksplisitt sier «som funksjon av _n_ ogW» er alt i orden. Men om vi
ikke oppgir noen eksplisitte parametre, så antas kjøretiden å være en funksjon _av
input-størrelsen_ , som altså er hvor stor plass en instans tar. Akkurat hvordan vi
måler denne størrelsen kan avhenge av problemet (se side 25 i læreboka), men når
vi regner på om ting kan løses i polynomisk tid (i forbindelse med NP-kompletthet
og denslags) holder vi oss til _antall bits_ i input, i en rimelig encoding. Størrelsen
blir daΘ( _n_ +lgW), siden vi bare trengerΘ(lgW)bits for å lagre parameterenW.
Poenget er altså atWvokser eksponentielt som funksjon avlgW, og kjøretiden er,
teknisk sett, eksponentiell.


Dette er kanskje enda enklere å se hvis vi lar _m_ være antall bits iW, så vi kan
skrive kjøretiden som
T( _n, m_ ) = Θ( _n_ 2 _m_ )_._

Da er det forhåpentligvis tydelig at dette ikke er en polynomisk kjøretid. Kjøretider
som er polynomiske hvis vi lar et _tall fra input_ være med som parameter til
kjøretiden (slik somΘ( _n_ W), derWer et tall fra input, og ikke direkte en del av
problemstørrelsen) kaller vi _pseudopolynomiske_.


## Appendiks-E

# Generell graftraversering

Læreboka beskriver to traverseringsalgoritmer, _bredde-først-søk_ (22.2) og _dybde-først-
søk_ (22.3). Disse fremstilles ganske ulikt, men er nært beslektede. Prosedyren BFS
(på side 595) kan tilpasses til å oppføre seg omtrent likt med DFS (side 605),
ved å bytte ut FIFO-køenQmed en LIFO-kø, eller stakk ( _stack_ ). Vi mister da
tidsmerkingen ( _v.d_ og _v.f_ ), men rekkefølgen noder farges grå og svarte på vil bli
den samme. (En annen forskjell er at DFS, slik den er beskrevet i boka, ikke har
noen startnode, men bare starter fra hver node etter tur, til den har nådd hele
grafen; sånn sett er BFS mer beslektet medDFS-Visit.)
KøenQi BFS er rett og slett en liste med noder vi har oppdaget via kanter fra
tidligere besøkte noder, men som vi ennå ikke har besøkt. Noder er hvite før de
legges inn, grå når de er iQog svarte etterpå. Det at vi bruker en FIFO-kø er det
som lar BFS finne de korteste stiene til alle noder, siden vi utforsker grafen «lagvis»
utover, men vi kan egentlig velge vilkårlige noder fraQi hver iterasjon, og vi vil
likevel traversere hele den delen av grafen vi kan nå fra startnoden.
Grunnen til at en LIFO-kø gir oss samme atferd som en rekursiv traversering
(altså DFS) er at vi egentlig bare simulerer hvordan rekursjon er implementert.
Internt bruker maskina en _kallstakk_ , der informasjon om hvert kall legges øverst, og
hentes frem igjen når rekursive kall er ferdige.
Som Cormen mfl. påpeker, så bruker Prims og Dijkstras algoritmer svært
lignende ideer. ProsedyreneMST-Prim(side 634) ogDijkstra(side 658) forenkler
ting litt ved å setteQ = G_._ Vfør løkken som brukerQ, så vi mister den delen
av traverseringen som handler om å _oppdage_ noder. I eldre fremstillinger av disse
algoritmene så ligner de mer på BFS, og noder legges inn iQnår de oppdages.
Den sentrale forskjellen er hvilke noder som tas _ut_ avQ, altså hvilke noder som
_prioriteres_. I BFS prioriteres de eldste og i DFS prioriteres de nyeste; iDijkstra
prioriteres noder med lavt avstandsestimat ( _v.d_ ), mens iPrimprioriteres noder
som har en lett kant til én av de besøkte (svarte) nodene. I tillegg oppdateres disse
prioritetene underveis.
Andre prioriteringer vil gi andre traverseringsalgoritmer, som for eksempel den
såkalteA∗-algoritmen (ikke pensum).


## Appendiks F

# Rekursjon og iterasjon

Iterative funksjoner kan oversettes til ekvivalente rekursive funksjoner, og omvendt.
Dette er noe vi ofte gjør under konstruksjon av algoritmer basert på rekursiv
dekomponering, kanskje spesielt innen dynamisk programmering. De to måtene
å tenke på kan være anvendelige i ulike situasjoner; for eksempel kan det være
praktisk å tenke rekursivt når man bygger løsninger fra del-løsninger, men iterative
løsninger unngår ekstrakostnader til funksjonskall, som kan være viktig når en
algoritme faktisk skal implementeres.

**Fra iterasjon til rekursjon**

Dette er en transisjon man ofte må gjøre om man skal lære seg såkalt funksjonell
programmering. Å leke seg med funksjonelle språk kan være nyttig for å bli grundig
vant til rekursjon.∗
Det enkleste å oversette direkte er kanskje løkker av typen _repeat-while_ (i mange
språk kjent som _do-while_ ), der betingelsen kommer til slutt. For eksempel:

1 repeat
2 body
3 whilecondition

Dette kan man oversette til:

Func( _vars_ )
1 body
2 ifcondition
3 Func( _vars_ )

Ett kall tilsvarer her én iterasjon av løkken, og _vars_ tilsvarer lokale variable som
sendes med fra iterasjon til iterasjon. La oss si vi f.eks. vil skrive ut tallene 1_... n_

1 _i_ = 1
2 repeat
3 print _i_
4 _i_ = _i_ + 1
5 while _i_ 6 _n_
∗Ta for eksempel en titt på Haskell (https://haskell.org).


Vi kan da skrive om det til:

```
Numbers( i, n )
1 print i
2 i = i + 1
3 if i 6 n
4 Numbers( i, n )
```
```
Funksjonen kalles da med argumenter 1 og n til å begynne med.
Intuitivt kan dette tolkes på flere måter. Én veldig enkel tolkning er at kallet til
Numbersforteller maskina at vi skal gå tilbake til start, og utføre definisjonen av
Numbers, akkurat som i en løkke. Det blir som om vi hadde skrevet om løkken til
det følgende (om vi antar at i alt er satt til 1 ):
```
```
1 print i
2 i = i + 1
3 if i 6 n
4 go to 1
```
```
Internt er det nettopp denne typen go to -utsagn kompilatoren oversetter løkker
til. I mange språk er det også nettopp slik kode man får om man bruker rekursjon
slik som vi har gjort over! Med andre ord: Både løkker og rekursjon kan ende med
samme primitive maskinkode, som inneholder verken løkker eller rekursjon.
En annen måte å forstå koden på, intuitivt, er at vi skiller mellom første iterasjon
og resten. Første iterasjon består av linje 1 og 2, og om vi ikke har kommet til
slutten, så utfører vi resten av iterasjonene rekursivt.
Om man trenger verdier som beregnes i løkken, kan disse returneres fra funksjo-
nen. For eksempel kan returverdiene være de samme variablene som før:
```
```
Func( vars )
1 body
2 ifcondition
3 vars =Func( vars )
4 return vars
```
Merk at nå kan ikke lenger kompilatoren uten videre oversette dette til et enkelt _go
to_ -utsagn (såkalt _tail-call optimization_ ) siden vi har kode etter det rekursive kallet.
La oss si at vi f.eks. vil _summere_ tallene fra 1 til _n_ , i stedet, med en løkke som:

```
1 s, i = 0 , 1
2 repeat
3 s = s + i
4 i = i + 1
5 while i 6 n
```
Vi kan skrive om dette til:


Sum( _i, n, s_ )
1 _s_ = _s_ + _i_
2 _i_ = _i_ + 1
3 if _i_ 6 _n_
4 _i, n, s_ =Sum( _i, n, s_ )
5 return _i, n, s_

Kallet som starter det hele blirSum(1 _, n,_ 0). (Her er det naturligvis unødvendig
å sende med _alle_ variablene tilbake, siden vi kun trenger _s_ , men det illustrerer at
oversettelsen her kan gjøres ganske mekanisk.)
Å håndtere andre typer løkker er relativt rett frem. For eksempel kan du begynne
med å skrive dem om til å være på _repeat-while_ -formen (selv om du nok ofte vil
kunne finne mer naturlige rekursive måter å skrive koden på).

**Fra rekursjon til iterasjon**

Som nevnt over: Hvis det rekursive kallet kommer helt til slutt i funksjonen, kan
vi ofte oversette det direkte til en ekvivalent løkke. Dersom koden fortsetter etter
et rekursivt kall, derimot, så må vi simulere rekursjonen på noe vis; vi må huske
hvilke verdier de lokale variablene hadde _før_ det rekursive kallet, og gi dem disse
verdiene igjen!
Måten vi gjør dette på er med en stakk, akkurat slik som rekursjon er imple-
mentert internt i programmeringsspråkene (vha. en _kallstakk_ ). La oss si vi har en
rekursiv funksjon som dette:

Func(_..._ )
1 before
2 ifcondition
3 Func(_..._ )
4 after

La oss si at koden både før og etter bruker de samme variablene, _vars_. Da kan vi
fjerne rekursjonen slik, derSer en LIFO-stakk:

1 repeat
2 before
3 Push(S, _vars_ )
4 whilecondition
5 whileSis not empty
6 _vars_ =Pop(S)
7 after

Det rekursive kallet tilFuncmottar formodentlig andre argumenter enn dem vi
får inn; det kan her bare håndteres med vanlige tilordninger.


# Bibliografi

[1] T. H. Cormen mfl. _Introduction to Algorithms_. 3. utg. The MIT Press, 2009.

[2] B. A. Oakley. _A mind for numbers: How to excel at math and science (even if
you flunked algebra)_. TarcherPerigee, 2014.


